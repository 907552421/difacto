//  @file   config.proto
//  @brief  the configuration for difacto
package difacto;

message Config {
  /// --  basic settings for input & output  --
  /// the task, which could be
  /// - train: the training task, which the default
  /// - predict: the prediction task
  /// - dist_train: distributed training
  optional string task = 0 [default = "train"];
  /// The input data, can be either a filename or a directory.
  optional string data_in = 1;
  /// The optional validation data set for a training task, it can be either a
  /// filename or a directory
  optional string val_data = 2;
  /// data format. default is libsvm
  optional string data_format = 3 [default = "libsvm"];
  /// the model output for a training task
  optional string model_out = 4;
  /// model input, should be specified if it is a prediction task, or a training
  /// task started from a previous model
  optional string model_in = 5;
  /// the filename for prediction output. should be specified for a prediction
  /// task.
  optional string predict_out = 6;


  optional uint64 feature_size = 7;

  /// -- basic settings for the linear model :math:`w`  --
  /// l1 regularizer for :math:`w`: :math:`\lambda_1 |w|_1`
  optional float l1 = 10 [default = 1];
  /// l2 regularizer for :math:`w`: :math:`\lambda_2 \|w\|_2^2`
  optional float l2 = 11 [default = 0];
  /// the learning rate :math:`\eta` (or :math:`\alpha`) for :math:`w`
  optional float lr = 12 [default = .01];

  /// -- basic settings for the embedding :math:`V` --
  /// the embedding dimension
  optional int32 V_dim = 21 [default = 10];
  /// the l2 regularizer for :math:`V`: :math:`\lambda_2 \|V_i\|_2^2`
  optional float V_l2 = 22;

  /// -- basic settings for the learning algorithm --
  /// the optimization algorithm
  optional string algo = 30 [default= 'sgd'];
  /// the minibatch size if using sgd.
  optional int32 minibatch = 31 [default = 1000];
  /// the maximal number of data passes
  optional int32 max_epoch = 32 [default = 10];
  /// stop earilier based on validation
  optional bool early_stop = 33 [default = false];

  /// -- advanced settings for input & output --
  /// save model for every k epoch. default is -1, which only saves for the
  /// last epoch
  optional int32 save_epoch = 40 [default = -1];
  /// load model from the k-th epoch. default is -1, which loads the last
  /// epoch model if model_in is set
  optional int32 load_iter = 41 [default = -1];
  /// for distributed training only. If true, then a worker will read all of the
  /// data specified by train_data and val_data instread of reading only a part
  /// of it. It is useful when the data has already dispatched into workers'
  /// local filesystem
  optional bool local_data = 42 [default = false];
  /// down sampling negative examples in the training data. no in default
  optional float neg_sampling = 104 [default = 1.0];
  /// randomly shuffle data for minibatch SGD. a minibatch is randomly picked from
  /// rand_shuffle * minibatch examples. default is 10.
  optional int32 rand_shuffle = 103 [default = 10];
  /// if true, then outputs a probability prediction. otherwise :math:`\langle
  /// x, y \rangle`
  optional bool prob_predict = 105 [default = true];
  /// virtually partition a file into n parts for better loadbalance. default is 10
  // optional int32 num_parts_per_file = 102 [default = 10];


  /// -- advanced settings for the linear model :math:`w`  --

  /// -- advanced settings for the embedding :math:`V` --
  /// use or not use the contraint :math:`V_i = 0` if :math:`w_i = 0`. yes in default
  optional bool l1_shrk = 114 [default = true];
  /// features with occurence < threshold (:math:`k`) will have no embedding
  optional int32 V_threshold = 2 [default = 10];

  /// learning rate :math:`\eta` for :math:`V`. if not specified, then share the
  /// same with :math:`w`
  optional float V_lr = 4 [default = .01];
  /// leanring rate :math:`\beta` for :math:`V`.
  optional float V_lr_beta = 5 [default = 1];
  /// apply dropout on the gradient of :math:`V`. no in default
  optional float V_dropout = 7 [default = 0];
  /// project the gradient of :math:`V` into :math:`[-c c]`. no in default
  optional float V_grad_clipping = 8 [default = 0];
  /// normalized the l2-norm of gradient of :math:`V`. no in default
  /// V is initialized by uniformly random weight in
  ///   [-init_scale, +init_scale]
  optional float V_init_scale = 6 [default = .01];

  optional float V_grad_normalization = 9 [default = 0];

  /// -- advanced settings for the learning algorithm --
  /// print the progress every n sec during training. 1 sec in default
  optional float print_sec = 111 [default = 1];
  /// learning rate :math:`\beta`, 1 in default
  optional float lr_beta = 112 [default = 1];
  /// the minimal objective decrease in early stop
  optional float min_objv_decr = 119 [default = .00001];
  /// the maximal allowed objective
  optional float max_objv = 118;

  /// - system performance -
  /// advanced setting for system performance
  /// number of threads used within a worker and a server
  optional int32 num_threads = 121 [default = 2];

  /// the maximal concurrent minibatches being processing at the same time for
  /// sgd, and the maximal concurrent blocks for block CD. 2 in default.
  // optional int32 max_concurrency = 122 [default = 2];
  /// cache the key list on both sender and receiver to reduce communication
  /// cost. it may increase the memory usage
  // optional bool key_cache = 123 [default = true];
  /// compression the message to reduce communication cost. it may increase the
  /// computation cost.
  // optional bool msg_compression = 124 [default = true];
  /// convert floating-points into fixed-point integers with n bytes. n can be 1,
  /// 2 and 3. 0 means no compression.
  // optional int32 fixed_bytes = 125 [default = 0];
}
